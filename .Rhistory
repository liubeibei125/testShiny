date<-c("20170425","20170426","20170427","20170428","20170429","20170430","20170501","20170502","20170503","20170504","20170505","20170506","20170507","20170508","20170509")
for(urlname in date){
dir.create(paste("E:/image/",urlname,"/",sep=""))
url<-paste("http://bcy.net/coser/toppost100?type=week&date=",urlname,sep="")
web<-getURL(url,httpheader=myHttpheader)
doc<- htmlTreeParse(web,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node<-getNodeSet(doc, "//div[@class='work-thumbnail__topBd']/a")
info=sapply(node,xmlGetAttr,"href")
web1<-as.character(web)
ru<-gregexpr("class=\"cut db fz16 text\".{1,50}</a>",web1)
text<-regmatches(web1,ru)
text<-unlist(text)
rules<-gregexpr("\n.{1,20}</a>",text)
text1<-regmatches(text,rules)
text1<-gsub("\n","",text1)
text1<-gsub("</a>","",text1)
x="http://bcy.net"
info1<-paste(x,info,sep="")
for(i in 1:100){
url1<-info1[i]
web2<-getURL(url1,httpheader=myHttpheader)
doc2<- htmlTreeParse(web2,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node2<-getNodeSet(doc2, "//img[@class='detail_std detail_clickable']")
info2=sapply(node1,xmlGetAttr,"src")
x5<-1
for(inf in info2){
dir.create(paste("E:/image/",urlname,"/",text1[i],sep=""))
y<-paste("E:/image/",urlname,"/",text1[i],"/",x5,".jpg",sep="")
tryCatch({
download.file(inf,y,mode="wb")
x<-x+1
},error=function(e){cat("ERROR:",conditionMessage(e),"\n")
print("los")
}
)}
}}
dir.create("E:/image/")
myHttpheader <- c("User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6)","Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8","Accept-Language"="en-us","Connection"="keep-alive","Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7")
date<-c("20170425","20170426","20170427","20170428","20170429","20170430","20170501","20170502","20170503","20170504","20170505","20170506","20170507","20170508","20170509")
for(urlname in date){
dir.create(paste("E:/image/",urlname,"/",sep=""))
url<-paste("http://bcy.net/coser/toppost100?type=week&date=",urlname,sep="")
web<-getURL(url,httpheader=myHttpheader)
doc<- htmlTreeParse(web,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node<-getNodeSet(doc, "//div[@class='work-thumbnail__topBd']/a")
info=sapply(node,xmlGetAttr,"href")
web1<-as.character(web)
ru<-gregexpr("class=\"cut db fz16 text\".{1,50}</a>",web1)
text<-regmatches(web1,ru)
text<-unlist(text)
rules<-gregexpr("\n.{1,20}</a>",text)
text1<-regmatches(text,rules)
text1<-gsub("\n","",text1)
text1<-gsub("</a>","",text1)
x="http://bcy.net"
info1<-paste(x,info,sep="")
for(i in 1:100){
url1<-info1[i]
web2<-getURL(url1,httpheader=myHttpheader)
doc2<- htmlTreeParse(web2,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node2<-getNodeSet(doc2, "//img[@class='detail_std detail_clickable']")
info2=sapply(node1,xmlGetAttr,"src")
x5<-1
for(inf in info2){
dir.create(paste("E:/image/",urlname,"/",text1[i],sep=""))
y<-paste("E:/image/",urlname,"/",text1[i],"/",x5,".jpg",sep="")
tryCatch({
download.file(inf,y)
x5<-x5+1
},error=function(e){cat("ERROR:",conditionMessage(e),"\n")
print("los")
}
)}
}}
dir.create("E:/image/")
myHttpheader <- c("User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6)","Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8","Accept-Language"="en-us","Connection"="keep-alive","Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7")
date<-c("20170425","20170426","20170427","20170428","20170429","20170430","20170501","20170502","20170503","20170504","20170505","20170506","20170507","20170508","20170509")
for(urlname in date){
dir.create(paste("E:/image/",urlname,"/",sep=""))
url<-paste("http://bcy.net/coser/toppost100?type=week&date=",urlname,sep="")
web<-getURL(url,httpheader=myHttpheader)
doc<- htmlTreeParse(web,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node<-getNodeSet(doc, "//div[@class='work-thumbnail__topBd']/a")
info=sapply(node,xmlGetAttr,"href")
web1<-as.character(web)
ru<-gregexpr("class=\"cut db fz16 text\".{1,50}</a>",web1)
text<-regmatches(web1,ru)
text<-unlist(text)
rules<-gregexpr("\n.{1,20}</a>",text)
text1<-regmatches(text,rules)
text1<-gsub("\n","",text1)
text1<-gsub("</a>","",text1)
x="http://bcy.net"
info1<-paste(x,info,sep="")
for(i in 1:100){
url1<-info1[i]
web2<-getURL(url1,httpheader=myHttpheader)
doc2<- htmlTreeParse(web2,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node2<-getNodeSet(doc2, "//img[@class='detail_std detail_clickable']")
info2=sapply(node1,xmlGetAttr,"src")
co<-1
for(inf in info2){
dir.create(paste("E:/image/",urlname,"/",text1[i],"/",sep=""))
y<-paste("E:/image/",urlname,"/",text1[i],"/",co,".jpg",sep="")
tryCatch({
download.file(inf,y,mode="wb")
co<-co+1
},error=function(e){cat("ERROR:",conditionMessage(e),"\n")
print("los")
}
)}
}}
inf
download.file(inf,y,mode="wb")
download.file(inf,"E:/image/1.jpg",mode="wb")
dir.create("E:/image/")
myHttpheader <- c("User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6)","Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8","Accept-Language"="en-us","Connection"="keep-alive","Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7")
date<-c("20170425","20170426","20170427","20170428","20170429","20170430","20170501","20170502","20170503","20170504","20170505","20170506","20170507","20170508","20170509")
for(urlname in date){
dir.create(paste("E:/image/",urlname,"/",sep=""))
url<-paste("http://bcy.net/coser/toppost100?type=week&date=",urlname,sep="")
web<-getURL(url,httpheader=myHttpheader)
doc<- htmlTreeParse(web,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node<-getNodeSet(doc, "//div[@class='work-thumbnail__topBd']/a")
info=sapply(node,xmlGetAttr,"href")
web1<-as.character(web)
ru<-gregexpr("class=\"cut db fz16 text\".{1,50}</a>",web1)
text<-regmatches(web1,ru)
text<-unlist(text)
rules<-gregexpr("\n.{1,20}</a>",text)
text1<-regmatches(text,rules)
text1<-gsub("\n","",text1)
text1<-gsub("</a>","",text1)
x="http://bcy.net"
info1<-paste(x,info,sep="")
for(i in 1:100){
url1<-info1[i]
web2<-getURL(url1,httpheader=myHttpheader)
doc2<- htmlTreeParse(web2,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node2<-getNodeSet(doc2, "//img[@class='detail_std detail_clickable']")
info2=sapply(node1,xmlGetAttr,"src")
co<-1
for(inf in info2){
dir.create(paste("E:/image/",urlname,"/",text1[i],"/",sep=""))
y<-paste("E:/image/",urlname,"/",text1[i],"/",co,".jpg",sep="")
tryCatch({
download.file(inf,y,mode="wb")
co<-co+1
},error=function(e){cat("ERROR:",conditionMessage(e),"\n")
print("los")
}
)}
}}
output$filee3 <- downloadHandler(
filename=paste0("mywendang", '.png') ,
content = function(file) {
jpeg(file)
ridan<-but5()   ##获取数据
qqnorm(ridan$单量)
qqline(ridan$单量)
dev.off()
})
output$filee2 <- downloadHandler(
filename=paste0("mywendang", '.png') ,
content = function(file) {
jpeg(file)
ridan<-but5()
hist(ridan$单量,freq = FALSE)
rug(ridan$单量)
lines(density(ridan$单量))
dev.off()
})
head(info1)
info[1]
info1[1]
dir.create("E:/image/")
myHttpheader <- c("User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6)","Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8","Accept-Language"="en-us","Connection"="keep-alive","Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7")
date<-c("20170425","20170426","20170427","20170428","20170429","20170430","20170501","20170502","20170503","20170504","20170505","20170506","20170507","20170508","20170509")
for(urlname in date){
dir.create(paste("E:/image/",urlname,"/",sep=""))
url<-paste("http://bcy.net/coser/toppost100?type=week&date=",urlname,sep="")
web<-getURL(url,httpheader=myHttpheader)
doc<- htmlTreeParse(web,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node<-getNodeSet(doc, "//div[@class='work-thumbnail__topBd']/a")
info=sapply(node,xmlGetAttr,"href")
web1<-as.character(web)
ru<-gregexpr("class=\"cut db fz16 text\".{1,50}</a>",web1)
text<-regmatches(web1,ru)
text<-unlist(text)
rules<-gregexpr("\n.{1,20}</a>",text)
text1<-regmatches(text,rules)
text1<-gsub("\n","",text1)
text1<-gsub("</a>","",text1)
x="http://bcy.net"
info1<-paste(x,info,sep="")
for(i in 1:100){
url1<-info1[i]
web2<-getURL(url1,httpheader=myHttpheader)
doc2<- htmlTreeParse(web2,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node2<-getNodeSet(doc2, "//img[@class='detail_std detail_clickable']")
info2=sapply(node2,xmlGetAttr,"src")
co<-1
for(inf in info2){
dir.create(paste("E:/image/",urlname,"/",text1[i],"/",sep=""))
y<-paste("E:/image/",urlname,"/",text1[i],"/",co,".jpg",sep="")
tryCatch({
download.file(inf,y,mode="wb")
co<-co+1
},error=function(e){cat("ERROR:",conditionMessage(e),"\n")
print("los")
}
)}
}}
info2
for(inf in info2){
dir.create(paste("E:/image/",urlname,"/",text1[i],"/",sep=""))
y<-paste("E:/image/",urlname,"/",text1[i],"/",co,".jpg",sep="")
tryCatch({
download.file(inf,y,mode="wb")
co<-co+1
},error=function(e){cat("ERROR:",conditionMessage(e),"\n")
print("los")
}
)}
download.file(inf,"E:/image/20170425/--阴阳师--姑获鸟--/1.jpg",mode="wb")
y<-paste("E:/image/",urlname,"/",text1[i],"/",co,".jpg",sep="")
y
x<-" fgf "
x
gsub(" ","",x)
dir.create("E:/image/")
myHttpheader <- c("User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6)","Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8","Accept-Language"="en-us","Connection"="keep-alive","Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7")
date<-c("20170425","20170426","20170427","20170428","20170429","20170430","20170501","20170502","20170503","20170504","20170505","20170506","20170507","20170508","20170509")
for(urlname in date){
dir.create(paste("E:/image/",urlname,"/",sep=""))
url<-paste("http://bcy.net/coser/toppost100?type=week&date=",urlname,sep="")
web<-getURL(url,httpheader=myHttpheader)
doc<- htmlTreeParse(web,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node<-getNodeSet(doc, "//div[@class='work-thumbnail__topBd']/a")
info=sapply(node,xmlGetAttr,"href")
web1<-as.character(web)
ru<-gregexpr("class=\"cut db fz16 text\".{1,50}</a>",web1)
text<-regmatches(web1,ru)
text<-unlist(text)
rules<-gregexpr("\n.{1,20}</a>",text)
text1<-regmatches(text,rules)
text1<-gsub("\n","",text1)
text1<-gsub("</a>","",text1)
x="http://bcy.net"
info1<-paste(x,info,sep="")
for(i in 1:100){
url1<-info1[i]
web2<-getURL(url1,httpheader=myHttpheader)
doc2<- htmlTreeParse(web2,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node2<-getNodeSet(doc2, "//img[@class='detail_std detail_clickable']")
info2=sapply(node2,xmlGetAttr,"src")
co<-1
for(inf in info2){
zhe<-gsub(" ","",text1[i])
dir.create(paste("E:/image/",urlname,"/",zhe,"/",sep=""))
y<-paste("E:/image/",urlname,"/",zhe,"/",co,".jpg",sep="")
tryCatch({
download.file(inf,y,mode="wb")
co<-co+1
},error=function(e){cat("ERROR:",conditionMessage(e),"\n")
print("los")
}
)}
}}
dir.create("E:/image/")
myHttpheader <- c("User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6)","Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8","Accept-Language"="en-us","Connection"="keep-alive","Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7")
date<-c("20170425","20170426","20170427","20170428","20170429","20170430","20170501","20170502","20170503","20170504","20170505","20170506","20170507","20170508","20170509")
for(urlname in date){
dir.create(paste("E:/image/",urlname,"/",sep=""))
tryCatch({
url<-paste("http://bcy.net/coser/toppost100?type=week&date=",urlname,sep="")
web<-getURL(url,httpheader=myHttpheader)
doc<- htmlTreeParse(web,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node<-getNodeSet(doc, "//div[@class='work-thumbnail__topBd']/a")
info=sapply(node,xmlGetAttr,"href")
},error=function(e){cat("ERROR:",conditionMessage(e),"\n")
print("los")
}
)
web1<-as.character(web)
ru<-gregexpr("class=\"cut db fz16 text\".{1,50}</a>",web1)
text<-regmatches(web1,ru)
text<-unlist(text)
rules<-gregexpr("\n.{1,20}</a>",text)
text1<-regmatches(text,rules)
text1<-gsub("\n","",text1)
text1<-gsub("</a>","",text1)
x="http://bcy.net"
info1<-paste(x,info,sep="")
for(i in 1:100){
url1<-info1[i]
tryCatch({
web2<-getURL(url1,httpheader=myHttpheader)
doc2<- htmlTreeParse(web2,encoding="UTF-8", error=function(...){}, useInternalNodes = TRUE,trim=TRUE)
node2<-getNodeSet(doc2, "//img[@class='detail_std detail_clickable']")
info2=sapply(node2,xmlGetAttr,"src")
},error=function(e){cat("ERROR:",conditionMessage(e),"\n")
print("los")
}
)
co<-1
for(inf in info2){
zhe<-gsub(" ","",text1[i])
dir.create(paste("E:/image/",urlname,"/",zhe,"/",sep=""))
y<-paste("E:/image/",urlname,"/",zhe,"/",co,".jpg",sep="")
tryCatch({
download.file(inf,y,mode="wb")
co<-co+1
},error=function(e){cat("ERROR:",conditionMessage(e),"\n")
print("los")
}
)}
}}
head(lun3)
p<-ggplot(lun3,aes(x=lun3$Group.1,y=lun3$x))
library(ggplot2)
p<-ggplot(lun3,aes(x=lun3$Group.1,y=lun3$x))
p + geom_point(aes(colour=factor(year)),alpha=0.5)+
stat_smooth()+
scale_color_manual(values =c('blue','red'))
p + geom_point(aes(colour=factor(lun3$Group.1)),alpha=0.5)+
stat_smooth()+
scale_color_manual(values =c('blue','red'))
p + geom_point()
p + geom_point()+ stat_smooth()
p + geom_line()
library(recharts)
eLine(dat=lun3,xvar = ~lun3$Group.1,yvar = ~lun3$x)
ts1<-ts(lun3$x,start = c(2007,1),frequency = 12)
ts1
ma3<-ma(ts,3)
library(forecast)
ma3<-ma(ts,3)
ma3<-ma(ts1,3)
ma3
eLine(ma3)
eLine(yvar=~ma3,xvar = ~lun3$Group.1)
eLine(dat=lun3,yvar=~ma3,xvar = ~lun3$Group.1)
title(main=""Simple Moving Averages (k=3)"")
title(main="Simple Moving Averages (k=3)")
title(main="Simple Moving Averages (k=3)")
title(main="Simple Moving Averages (k=3)")
eLine(dat=lun3,yvar=~ma3,xvar = ~lun3$Group.1,title = "Simple Moving Averages (k=3)")
ma15<-ma(ts1,15)
eLine(dat=lun3,yvar=~ma15,xvar = ~lun3$Group.1,title = "Simple Moving Averages (k=15)")
par(mfrow=c(2,1))
library(forecast)
monthplot(ts1, xlab="", ylab="")
seasonplot(ts1, year.labels="TRUE", main="")
par(mfrow=c(2,1))
library(forecast)
monthplot(ts1, xlab="", ylab="")
seasonplot(ts1, year.labels="TRUE", main="")
library(forecast)
monthplot(ts1, xlab="", ylab="")
seasonplot(ts1, year.labels="TRUE", main="")
seasonplot(ts1, year.labels="TRUE", main="")
head(lun2)
head(lun1)
library(xts)
ts2<-xts(lun1$mean1,lun1$date)
ts2
eLine(ts2)
dt<-diff(ts2)
eLine(dt)
library(tseries)
adf.test(dt)
adf.test(dt[-1])
acf(dt[-1])
pacf(dt[-1])
arima(ts2,order=c(3,1,3))
arima(ts2,order=c(3,1,4))
Box.test(fit$residuals,type="Ljung-Box")
fit<-arima(ts2,order=c(3,1,4))
Box.test(fit$residuals,type="Ljung-Box")
shiny::runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
tail(zz1)
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
x<-c("2017-05-01","2017-05-05")
x[2]-x[1]
x<-as.Date(x)
x[2]-x[1]
as.numeric(x[2]-x[1])
runApp('G:/demo/森普shiny')
zz
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
rm
head(send_order)
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
Conn<-dbConnect(MySQL(),dbname="server-kkpb",username="tongji",password="fS8h9YbY",host="rr-2zed6zah153z846fho.mysql.rds.aliyuncs.com",port=3306)
dbSendQuery(Conn,"SET NAMES gbk")
dates7<-c("2017-05-01","2017-05-03")
sql2<-paste("SELECT comment_score,express_id,FROM_UNIXTIME(add_time) as x,order_sn FROM kkpb_comments WHERE date(from_unixtime(add_time))>=\'",dates7[1],"\'"," AND DATE(FROM_UNIXTIME(add_time))<=\'",dates7[2],"\'"," AND type=1",sep="")
print(sql2)
comment<-dbGetQuery(Conn,sql2)
print(dim(comment))
comment$x1<-as.Date(comment$x)
comment
View(comment)
View(comment)
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
runApp('G:/demo/森普shiny')
rm(daily_order)
library(xlsx)
write.xlsx(x=lun3,file="C:/Users/Administrator/Desktop/lun3.xlsx")
write.xlsx(x=lun1,file="C:/Users/Administrator/Desktop/lun1.xlsx")
fit<-arima(ts1,order=c(3,1,4))
fit
library(xts)
ts1<-xts(lun1$mean1,lun1$date)
fit<-arima(ts1,order=c(3,1,4))
fit
x.fore<-forecast(fit,5)
x.fore<-forecast(fit,5)
library(forecast)
x.fore<-forecast(fit,5)
x.fore$fitted
length(ts1)
forecast(fit,5)
tail(x.fore$fitted)
fitted(fit)
x<-fitted(fit)
y<-lun1$mean1
length(x)
length(y)
x-y
abs(x-y)
mean(abs(x-y))
forecast(fit,5)
zz<-forecast(fit,5)
zz
zz<-as.data.frame(zz)
zz$`Point Forecast`
x<-c(x,zz$`Point Forecast`)
length(x)
length(y)
da<-lun1$date
da<-c(da,"2017-01-03","2017-01-04","2017-01-05","2017-01-06","2017-01-09")
plot(x)
plot(x,type="s")
plot(x,type="s")
line(y)
head(y)
aline(y)
lines(y)
plot(x,type="s",ylab = da)
plot(x,type="s",ylim  = da)
plot(x,type="s",yaxt=“n”)
plot(x,type="s",yaxt="n")
plot(x,type="s",xaxt="n")
par(xlim=c(1,2437),xlab=da)
plot(x,type="s",xaxt="n",xlim=c(1,2437),xlab=da)
data<-data.frame(a=x,b=y,c=da)
data<-data.frame(a=x,c=da)
head(data)
plot(data$a,ylim = )
plot(data$a,xlab = data$c )
dat<-data
plot(dat)
plot(dat$c,dat$a)
plot(dat$c,dat$a,type="s")
tail(dat)
lines(y)
plot(dat$c,dat$a)
lines(y)
length(y)
length(dat$a)
plot(dat$c,dat$a)
plot(dat$c,dat$a,type="s")
head(z)
tail(z)
rm(z)
View(zz)
tail(y)
tail(lun1)
shiny::runApp('G:/demo/senshiny')
express_id
runApp('G:/demo/senshiny')
where(i==1){print(ss)}
shiny::runApp()
shiny::runApp()
